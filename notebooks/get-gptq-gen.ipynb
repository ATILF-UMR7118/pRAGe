{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8518943,"sourceType":"datasetVersion","datasetId":4931502}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install auto-gptq\n!pip install optimum\n!pip install bitsandbytes\n!pip install peft","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, MistralForCausalLM\nfrom peft import prepare_model_for_kbit_training\nfrom peft import LoraConfig, get_peft_model #prepare_model_for_int8_training\nfrom datasets import load_dataset\nimport transformers\n\nfrom huggingface_hub import hf_hub_download\n\nfrom tqdm import tqdm\nimport time\nimport torch","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-29T04:53:09.864198Z","iopub.execute_input":"2024-05-29T04:53:09.864879Z","iopub.status.idle":"2024-05-29T04:53:18.178947Z","shell.execute_reply.started":"2024-05-29T04:53:09.864835Z","shell.execute_reply":"2024-05-29T04:53:18.177971Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-05-29 04:53:13.144394: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-29 04:53:13.144452: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-29 04:53:13.145967: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"model_name = \"LoneStriker/BioMistral-7B-SLERP-GPTQ\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name,\n                                             device_map=\"auto\", # automatically figures out how to best use CPU + GPU for loading model\n                                             trust_remote_code=False, # prevents running custom model files on your machine\n                                             revision=\"main\")\n\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T04:53:21.211170Z","iopub.execute_input":"2024-05-29T04:53:21.211945Z","iopub.status.idle":"2024-05-29T04:53:51.409719Z","shell.execute_reply.started":"2024-05-29T04:53:21.211903Z","shell.execute_reply":"2024-05-29T04:53:51.408934Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/4.57G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"246a120c47814dac8548d786ef7d187e"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:4225: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41ecf3d8127e44fa9856aedc7e0a8cba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.47k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa4bf074bae0436791355a035b6ca942"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef37251a79ed4945aac98340ae90be62"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96b274b8d1b84617a12f3fad1630b5b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9eeb863fe4cc4bd1b8441d9d95d1f5cf"}},"metadata":{}}]},{"cell_type":"markdown","source":"## DEMO EVAL","metadata":{}},{"cell_type":"code","source":"model.eval() # model in evaluation mode (dropout modules are deactivated)\n\n# craft prompt\ncomment = \"trouble digestif?\"\nprompt=f'''[INST] Explique moi en francais : {comment} [/INST]'''\n\n# tokenize input\ninputs = tokenizer(prompt, return_tensors=\"pt\")\n\n# generate output\noutputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=50)\n\nprint(tokenizer.batch_decode(outputs)[0])","metadata":{"execution":{"iopub.status.busy":"2024-05-29T04:54:24.141751Z","iopub.execute_input":"2024-05-29T04:54:24.142137Z","iopub.status.idle":"2024-05-29T04:55:19.756505Z","shell.execute_reply.started":"2024-05-29T04:54:24.142105Z","shell.execute_reply":"2024-05-29T04:55:19.755065Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"<s> [INST] Explique moi en francais : trouble digestif? [/INST] Le trouble digestif désigne un groupe de symptômes ou de maladies qui affectent le fonctionnement du système digestif. Ce système est responsable de la digestion des aliments et de leur transformation en nutriments\n","output_type":"stream"}]},{"cell_type":"code","source":"transformers.__version__","metadata":{"execution":{"iopub.status.busy":"2024-05-29T04:55:19.758392Z","iopub.execute_input":"2024-05-29T04:55:19.758876Z","iopub.status.idle":"2024-05-29T04:55:19.765333Z","shell.execute_reply.started":"2024-05-29T04:55:19.758838Z","shell.execute_reply":"2024-05-29T04:55:19.764463Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"'4.39.3'"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\nvaldf = pd.read_csv('/kaggle/input/refomed/refomed_val.csv', sep='\\t')\ntestdf = pd.read_csv('/kaggle/input/refomed/refomed_test.csv', sep='\\t')\ntraindf= pd.read_csv('/kaggle/input/refomed/refomed_train.csv', sep='\\t')","metadata":{"execution":{"iopub.status.busy":"2024-05-29T04:55:19.766500Z","iopub.execute_input":"2024-05-29T04:55:19.766783Z","iopub.status.idle":"2024-05-29T04:55:19.832745Z","shell.execute_reply.started":"2024-05-29T04:55:19.766759Z","shell.execute_reply":"2024-05-29T04:55:19.831923Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"prompts = [\"Expliquez-moi, en termes simples ou paraphrase ou définition :\",\n\"Expliquez-moi le terme médical en mots simples, par une paraphrase ou une courte définition :\",\n]\n\nqueries1 = []\nfor k in testdf.term:\n    queries1.append('[INST]' + prompts[1]+f' {k}'+ '[/INST]')\n\nfrom datasets import Dataset\n\ntestdata = Dataset.from_pandas(pd.DataFrame(queries1, columns=['prompt']))\n\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.truncation_side = \"right\"\ntokenizer.padding_side = \"left\" \ndef tokenize_function(examples):\n    # extract text\n    text = examples[\"prompt\"]\n    #tokenize and truncate text\n    #tokenizer.truncation_side = \"right\"\n    tokenized_inputs = tokenizer(\n        text,\n        return_tensors=\"pt\",\n        #truncation=True,\n        #padding=True,\n        padding='max_length',\n        max_length=512\n    )\n    return tokenized_inputs\n\ntokenized_testdata = testdata.map(tokenize_function, batched=True)\nprint(tokenized_testdata)\nprint(tokenized_testdata[4])","metadata":{"execution":{"iopub.status.busy":"2024-05-29T04:55:19.834789Z","iopub.execute_input":"2024-05-29T04:55:19.835477Z","iopub.status.idle":"2024-05-29T04:55:20.537021Z","shell.execute_reply.started":"2024-05-29T04:55:19.835439Z","shell.execute_reply":"2024-05-29T04:55:20.536089Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1253 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a523e60948354eadb42be1ea2af3d519"}},"metadata":{}},{"name":"stdout","text":"Dataset({\n    features: ['prompt', 'input_ids', 'attention_mask'],\n    num_rows: 1253\n})\n{'prompt': \"[INST]Expliquez-moi le terme médical en mots simples, par une paraphrase ou une courte définition : céphalée d'allure commune[/INST]\", 'input_ids': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 733, 16289, 28793, 966, 452, 1651, 28764, 28733, 28719, 3950, 462, 1850, 28706, 15454, 745, 481, 290, 1649, 1290, 2815, 28725, 940, 2219, 940, 377, 2176, 555, 3466, 2219, 1547, 424, 2306, 3013, 685, 714, 277, 28797, 721, 282, 2110, 281, 28742, 455, 482, 11564, 28792, 28748, 16289, 28793], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-05-29T04:55:20.538145Z","iopub.execute_input":"2024-05-29T04:55:20.538427Z","iopub.status.idle":"2024-05-29T04:55:20.542722Z","shell.execute_reply.started":"2024-05-29T04:55:20.538399Z","shell.execute_reply":"2024-05-29T04:55:20.541853Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Define a collate function to pad inputs dynamically\ndef collate_fn(batch):\n    input_ids = torch.nn.utils.rnn.pad_sequence([torch.tensor(item['input_ids']) for item in batch], batch_first=True)\n    attention_mask = torch.nn.utils.rnn.pad_sequence([torch.tensor(item['attention_mask']) for item in batch], batch_first=True)\n    return {'input_ids': input_ids, 'attention_mask': attention_mask}\n\nbatch_size = 32  # Define your batch size\nfrom torch.utils.data import DataLoader\ndata_loader = DataLoader(tokenized_testdata, batch_size=batch_size, collate_fn=collate_fn)\ndata_loader\n\nmodel.eval()\ndevice = 'cuda'\nfor T in [25,50]:\n    responses2, times2 = [],[]\n\n    for batch in tqdm(data_loader):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        #print(input_ids.shape)\n        t1 = time.time()\n        outputs = model.generate(input_ids=input_ids, \n                             attention_mask = attention_mask,\n                             max_new_tokens=T)\n        #print(outputs)\n        answer = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n        t2 = time.time()\n        responses2.extend(answer)\n        times2.extend([(t2-t1)/input_ids.shape[0]]*input_ids.shape[0])\n        #break\n\n    pd.DataFrame({\"prompts\":tokenized_testdata['prompt'], \n                      \"zsa\": responses2, \n                      \"time_taken\":times2}).to_csv(f'final_qptq_biom_generation-p4-test-t{T}.csv', sep='\\t', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T04:55:39.306647Z","iopub.execute_input":"2024-05-29T04:55:39.307741Z","iopub.status.idle":"2024-05-29T06:15:12.483399Z","shell.execute_reply.started":"2024-05-29T04:55:39.307695Z","shell.execute_reply":"2024-05-29T06:15:12.482345Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"  0%|          | 0/40 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n  2%|▎         | 1/40 [00:43<28:22, 43.66s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n  5%|▌         | 2/40 [01:28<27:55, 44.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n  8%|▊         | 3/40 [02:12<27:15, 44.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 10%|█         | 4/40 [02:56<26:36, 44.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 12%|█▎        | 5/40 [03:41<25:53, 44.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 15%|█▌        | 6/40 [04:25<25:11, 44.44s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 18%|█▊        | 7/40 [05:10<24:27, 44.47s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 20%|██        | 8/40 [05:55<23:44, 44.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 22%|██▎       | 9/40 [06:39<22:59, 44.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 25%|██▌       | 10/40 [07:24<22:15, 44.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 28%|██▊       | 11/40 [08:08<21:31, 44.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 30%|███       | 12/40 [08:53<20:46, 44.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 32%|███▎      | 13/40 [09:37<20:01, 44.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 35%|███▌      | 14/40 [10:22<19:17, 44.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 38%|███▊      | 15/40 [11:06<18:32, 44.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 40%|████      | 16/40 [11:51<17:48, 44.53s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 42%|████▎     | 17/40 [12:35<17:03, 44.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 45%|████▌     | 18/40 [13:20<16:19, 44.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 48%|████▊     | 19/40 [14:04<15:34, 44.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 50%|█████     | 20/40 [14:49<14:50, 44.50s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 52%|█████▎    | 21/40 [15:33<14:05, 44.48s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 55%|█████▌    | 22/40 [16:18<13:20, 44.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 57%|█████▊    | 23/40 [17:02<12:36, 44.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 60%|██████    | 24/40 [17:47<11:51, 44.50s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 62%|██████▎   | 25/40 [18:31<11:07, 44.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 65%|██████▌   | 26/40 [19:16<10:23, 44.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 68%|██████▊   | 27/40 [20:00<09:38, 44.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 70%|███████   | 28/40 [20:45<08:54, 44.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 72%|███████▎  | 29/40 [21:29<08:09, 44.53s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 75%|███████▌  | 30/40 [22:14<07:25, 44.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 78%|███████▊  | 31/40 [22:58<06:40, 44.53s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 80%|████████  | 32/40 [23:43<05:56, 44.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 82%|████████▎ | 33/40 [24:27<05:11, 44.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 85%|████████▌ | 34/40 [25:12<04:27, 44.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 88%|████████▊ | 35/40 [25:56<03:42, 44.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 90%|█████████ | 36/40 [26:41<02:58, 44.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 92%|█████████▎| 37/40 [27:25<02:13, 44.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 95%|█████████▌| 38/40 [28:10<01:29, 44.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 98%|█████████▊| 39/40 [28:54<00:44, 44.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n100%|██████████| 40/40 [29:24<00:00, 44.12s/it]\n  0%|          | 0/40 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n  2%|▎         | 1/40 [01:15<49:08, 75.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n  5%|▌         | 2/40 [02:31<47:53, 75.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n  8%|▊         | 3/40 [03:46<46:37, 75.62s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 10%|█         | 4/40 [05:02<45:23, 75.66s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 12%|█▎        | 5/40 [06:18<44:08, 75.66s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 15%|█▌        | 6/40 [07:33<42:53, 75.68s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 18%|█▊        | 7/40 [08:49<41:37, 75.68s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 20%|██        | 8/40 [10:05<40:21, 75.68s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 22%|██▎       | 9/40 [11:20<39:05, 75.67s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 25%|██▌       | 10/40 [12:36<37:49, 75.65s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 28%|██▊       | 11/40 [13:52<36:34, 75.67s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 30%|███       | 12/40 [15:07<35:18, 75.67s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 32%|███▎      | 13/40 [16:23<34:02, 75.66s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 35%|███▌      | 14/40 [17:39<32:47, 75.68s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 38%|███▊      | 15/40 [18:55<31:31, 75.68s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 40%|████      | 16/40 [20:10<30:16, 75.68s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 42%|████▎     | 17/40 [21:26<29:00, 75.67s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 45%|████▌     | 18/40 [22:41<27:44, 75.67s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 48%|████▊     | 19/40 [23:57<26:29, 75.68s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 50%|█████     | 20/40 [25:13<25:13, 75.69s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 52%|█████▎    | 21/40 [26:29<23:58, 75.70s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 55%|█████▌    | 22/40 [27:44<22:42, 75.71s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 57%|█████▊    | 23/40 [29:00<21:26, 75.70s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 60%|██████    | 24/40 [30:16<20:11, 75.69s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 62%|██████▎   | 25/40 [31:31<18:55, 75.71s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 65%|██████▌   | 26/40 [32:47<17:40, 75.74s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 68%|██████▊   | 27/40 [34:03<16:24, 75.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 70%|███████   | 28/40 [35:19<15:08, 75.71s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 72%|███████▎  | 29/40 [36:34<13:53, 75.74s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 75%|███████▌  | 30/40 [37:50<12:37, 75.74s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 78%|███████▊  | 31/40 [39:06<11:21, 75.74s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 80%|████████  | 32/40 [40:22<10:05, 75.73s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 82%|████████▎ | 33/40 [41:37<08:50, 75.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 85%|████████▌ | 34/40 [42:53<07:34, 75.71s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 88%|████████▊ | 35/40 [44:09<06:18, 75.71s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 90%|█████████ | 36/40 [45:24<05:02, 75.73s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 92%|█████████▎| 37/40 [46:40<03:47, 75.73s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 95%|█████████▌| 38/40 [47:56<02:31, 75.65s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 98%|█████████▊| 39/40 [49:11<01:15, 75.47s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n100%|██████████| 40/40 [50:08<00:00, 75.21s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\ndevice = 'cuda'\n\nfor t in [25,50]:\n    responses2, times2 = [],[]\n    for d in tqdm(tokenized_testdata):\n        #print(d)\n        t1 = time.time()\n        outputs = model.generate(input_ids=torch.tensor([d[\"input_ids\"]]).to(\"cuda\"), \n                             attention_mask = torch.tensor([d[\"attention_mask\"]]).to(\"cuda\"),\n                             max_new_tokens=t)\n        answer = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n        t2 = time.time()\n        responses2.append(answer)\n        times2.append(t2-t1)\n\n    pd.DataFrame({\"prompts\":tokenized_testdata['prompt'], \n                  \"zsa\": responses2, \n                  \"time_taken\":times2}).to_csv(f'final_qptq_biom_generation-p4-test-t50.csv', sep='\\t', index=False)","metadata":{},"execution_count":null,"outputs":[]}]}